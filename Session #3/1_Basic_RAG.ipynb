{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lElF3o5PR6ys"
   },
   "source": [
    "# Basic RAG - From Scratch!!\n",
    "\n",
    "In this notebook, we'll walk you through each of the components that are involved in a simple RAG application.\n",
    "\n",
    "We won't be leveraging any fancy tools, just the OpenAI Python SDK, Numpy, and some classic Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Activity #1: First let's explore embeddings and cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vec_1, vec_2):\n",
    "  return np.dot(vec_1, vec_2) / (norm(vec_1) * norm(vec_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the `text-embedding-3-small` embedding model (more on that in a second) to embed two sentences. In order to use this embedding model endpoint - we'll need to provide our OpenAI API key!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from getpass import getpass\n",
    "\n",
    "openai.api_key = getpass(\"OpenAI API Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, define the embedding model. Credit goes to AI Makerspace for the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psi.openai_utils.embedding import EmbeddingModel\n",
    "\n",
    "embedding_model = EmbeddingModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's explore cosine similiarity of 2 embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.3407870581002385)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puppy_sentence = \"I love puppies!\"\n",
    "dog_sentence = \"I hate LLMs!\"\n",
    "puppy_vector = embedding_model.get_embedding(puppy_sentence)\n",
    "dog_vector = embedding_model.get_embedding(dog_sentence)\n",
    "cosine_similarity(puppy_vector, dog_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### 🎯 Breakout Room - Group Discussion: \n",
    "\n",
    "Explore how cosine similarity works and discuss your observations with the group. Write down final answer below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### Observations :\n",
    "\n",
    "* Cosine similarity works as a metric used to measure how similar two vectors are, regardless of their magnitude. In the context of RAG (Retrieval Augmented Generation), cosine similarity is being utilized to compare text(embeddings), find relevant context given a chunk of documents, given a user query, and capture semantic understanding through cosine similarity. We can observe that we used cosine similarity to compare the embeddings of similar sentences, even though they use different words, but express the same sentiments, resulting in a high cosine similarity.\n",
    "</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CtcL8P8R6yt"
   },
   "source": [
    "## Let's RAG-ing Time!\n",
    "\n",
    "- Task 1: Imports and Utilities\n",
    "- Task 2: Documents\n",
    "- Task 3: Embeddings and Vectors\n",
    "- Task 4: Prompts\n",
    "- Task 5: Retrieval Augmented Generation\n",
    "  - 🚧 Activity #1: Augment RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Dz6GYilR6yt"
   },
   "source": [
    "Here's a more visual representation of the task to be performed.\n",
    "\n",
    "![RAG Steps](images/ragging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjmC0KFtR6yt"
   },
   "source": [
    "## Import Wall\n",
    "\n",
    "We're just doing some imports and enabling `async` to work within the Jupyter environment here, nothing too crazy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Z1dyrG4hR6yt"
   },
   "outputs": [],
   "source": [
    "from psi.text_utils import TextFileLoader, CharacterTextSplitter\n",
    "from psi.vectordatabase import VectorDatabase\n",
    "import asyncio\n",
    "\n",
    "#  need  this support async in jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SFPWvRUR6yu"
   },
   "source": [
    "### Step 1: Loading Source Documents\n",
    "\n",
    "So, first things first, we need some documents to work with.\n",
    "\n",
    "While we could work directly with the `.txt` files (or whatever file-types you wanted to extend this to) we can instead do some batch processing of those documents at the beginning in order to store them in a more machine compatible format.\n",
    "\n",
    "In this case, we're going to parse our text file into a single document in memory.\n",
    "\n",
    "Let's look at the relevant bits of the `TextFileLoader` class:\n",
    "\n",
    "```python\n",
    "def load_file(self):\n",
    "        with open(self.path, \"r\", encoding=self.encoding) as f:\n",
    "            self.documents.append(f.read())\n",
    "```\n",
    "\n",
    "We're simply loading the document using the built in `open` method, and storing that output in our `self.documents` list.\n",
    "\n",
    "> NOTE: We're using text from Noli Me Tangere as our sample data. This data is largely irrelevant as we want to focus on the mechanisms of RAG, which includes out data's shape and quality - but not specifically what the contents of the data are. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ia2sUEuGR6yu",
    "outputId": "84937ecc-c35f-4c4a-a4ab-9da72625954c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_loader = TextFileLoader(\"data/noli.txt\")\n",
    "documents = text_loader.load_documents()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bV-tj5WFR6yu",
    "outputId": "674eb315-1ff3-4597-bcf5-38ece0a812ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Beginning (characters 0-100)\n",
      "==================================================\n",
      "The Project Gutenberg eBook of The Social Cancer: A Complete English Version of Noli Me Tangere\n",
      "    \n",
      "\n",
      "==================================================\n",
      "Middle Section (characters 2000-2500)\n",
      "==================================================\n",
      "orical sketches. The reader\n",
      "    flies in his express train in a few minutes through a couple\n",
      "    of centuries. The centuries pass more slowly to those to\n",
      "    whom the years are doled out day by day. Institutions grow\n",
      "    and beneficently develop themselves, making their way into\n",
      "    the hearts of generations which are shorter-lived than they,\n",
      "    attracting love and respect, and winning loyal obedience;\n",
      "    and then as gradually forfeiting by their shortcomings\n",
      "    the allegiance which had been \n"
     ]
    }
   ],
   "source": [
    "def print_document_portion(doc, start, end, title=\"Document Portion\"):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{title} (characters {start}-{end})\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(doc[start:end])\n",
    "\n",
    "# Usage\n",
    "print_document_portion(documents[0], 0, 100, \"Beginning\")\n",
    "print_document_portion(documents[0], 2000, 2500, \"Middle Section\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHlTvCzYR6yu"
   },
   "source": [
    "### Splitting Text Into Chunks\n",
    "\n",
    "As we can see, there is one massive document.\n",
    "\n",
    "We'll want to chunk the document into smaller parts so it's easier to pass the most relevant snippets to the LLM.\n",
    "\n",
    "There is no fixed way to split/chunk documents - and you'll need to rely on some intuition as well as knowing your data *very* well in order to build the most robust system.\n",
    "\n",
    "For this toy example, we'll just split blindly on length.\n",
    "\n",
    ">There's an opportunity to clear up some terminology here, for this course we will be stick to the following:\n",
    ">\n",
    ">- \"source documents\" : The `.txt`, `.pdf`, `.html`, ..., files that make up the files and information we start with in its raw format\n",
    ">- \"document(s)\" : single (or more) text object(s)\n",
    ">- \"corpus\" : the combination of all of our documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2G6Voc0jR6yv"
   },
   "source": [
    "As you can imagine (though it's not specifically true in this toy example) the idea of splitting documents is to break them into managable sized chunks that retain the most relevant local context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMC4tsEmR6yv",
    "outputId": "08689c0b-57cd-4040-942a-8193e997f5cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1283"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter()\n",
    "split_documents = text_splitter.split_texts(documents)\n",
    "len(split_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2wKT0WLR6yv"
   },
   "source": [
    "Let's take a look at some of the documents we've managed to split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vcYMwWJoR6yv",
    "outputId": "20d69876-feca-4826-b4be-32915276987a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg eBook of The Social Cancer: A Complete English Version of Noli Me Tangere\\n    \\nThis ebook is for the use of anyone anywhere in the United States and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever. You may copy it, give it away or re-use it under the terms\\nof the Project Gutenberg License included with this ebook or online\\nat www.gutenberg.org. If you are not located in the United States,\\nyou will have to check the laws of the country where you are located\\nbefore using this eBook.\\n\\nTitle: The Social Cancer: A Complete English Version of Noli Me Tangere\\n\\nAuthor: José Rizal\\n\\nTranslator: Charles E. Derbyshire\\n\\nRelease date: October 1, 2004 [eBook #6737]\\n                Most recently updated: April 13, 2010\\n\\nLanguage: English\\n\\nCredits: Produced by Jeroen Hellingman.\\n\\n\\n*** START OF THE PROJECT GUTENBERG EBOOK THE SOCIAL CANCER: A COMPLETE ENGLISH VERSION OF NOLI ME TANGERE ***\\n\\n\\n\\n\\nProduced by Jeroen Hellingman.\\n\\n\\n\\n\\n\\n\\n\\n\\n              ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_documents[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOU-RFP_R6yv"
   },
   "source": [
    "## Step 2: Compute Embeddings per Chunk\n",
    "\n",
    "Next, we have to convert our corpus into a \"machine readable\" format as we explored in the Embedding Primer notebook.\n",
    "\n",
    "Today, we're going to talk about the actual process of creating, and then storing, these embeddings, and how we can leverage that to intelligently add context to our queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Database\n",
    "\n",
    "Let's set up our vector database to hold all our documents and their embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDQrfAR1R6yv"
   },
   "source": [
    "While this is all baked into 1 call - we can look at some of the code that powers this process to get a better understanding:\n",
    "\n",
    "Let's look at our `VectorDatabase().__init__()`:\n",
    "\n",
    "```python\n",
    "def __init__(self, embedding_model: EmbeddingModel = None):\n",
    "        self.vectors = defaultdict(np.array)\n",
    "        self.embedding_model = embedding_model or EmbeddingModel()\n",
    "```\n",
    "\n",
    "As you can see - our vectors are merely stored as a dictionary of `np.array` objects.\n",
    "\n",
    "Secondly, our `VectorDatabase()` has a default `EmbeddingModel()` which is a wrapper for OpenAI's `text-embedding-3-small` model.\n",
    "\n",
    "> **Quick Info About `text-embedding-3-small`**:\n",
    "> - It has a context window of **8191** tokens\n",
    "> - It returns vectors with dimension **1536**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L273pRdeR6yv"
   },
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "#### ❓Question #1:\n",
    "\n",
    "The default embedding dimension of `text-embedding-3-small` is 1536, as noted above. \n",
    "\n",
    "1. Is there any way to modify this dimension?\n",
    "2. What technique does OpenAI use to achieve this?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### Answer :\n",
    "\n",
    "* Answer to Question #1: To modify the embedding dimension, we can specify the dimension by passing it in the dimensions parameter of the API.\n",
    "* Answer to Question #2: To achieve this, OpenAI utilizes Matryoshka Representation Learning (MRL) that enables a model to output representations of varying sizes.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: Check out this [API documentation](https://platform.openai.com/docs/api-reference/embeddings/create) for the answer to question #1, and [this documentation](https://platform.openai.com/docs/guides/embeddings/use-cases) for an answer to question #2!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5FZY7K3R6yv"
   },
   "source": [
    "We can call the `async_get_embeddings` method of our `EmbeddingModel()` on a list of `str` and receive a list of `float` back!\n",
    "\n",
    "```python\n",
    "async def async_get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
    "        return await aget_embeddings(\n",
    "            list_of_text=list_of_text, engine=self.embeddings_model_name\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSct6X0aR6yv"
   },
   "source": [
    "We cast those to `np.array` when we build our `VectorDatabase()`:\n",
    "\n",
    "```python\n",
    "async def abuild_from_list(self, list_of_text: List[str]) -> \"VectorDatabase\":\n",
    "        embeddings = await self.embedding_model.async_get_embeddings(list_of_text)\n",
    "        for text, embedding in zip(list_of_text, embeddings):\n",
    "            self.insert(text, np.array(embedding))\n",
    "        return self\n",
    "```\n",
    "\n",
    "And that's all we need to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = VectorDatabase()\n",
    "vector_db = asyncio.run(vector_db.abuild_from_list(split_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSZwaGvpR6yv"
   },
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### ❓ Assignment Question #2:\n",
    "\n",
    "What are the benefits of using an `async` approach to collecting our embeddings?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: Determining the core difference between `async` and `sync` will be useful! If you get stuck - ask ChatGPT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### Answer:\n",
    "\n",
    "* The benefits of using an `async` approach to collecting embeddings include concurrency and improved performance, allowing for multiple API requests to be made simultaneously, significantly reducing the total processing time when handling a large volume of documents. Additionally, it enables non-blocking operations, meaning the program doesn't freeze and remains responsive while waiting for API responses; other tasks can continue running as embedding requests are processed. Another benefit is scalability; it can handle hundreds or thousands of documents more efficiently.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRBdIt-xR6yw"
   },
   "source": [
    "So, to review what we've done so far in natural language:\n",
    "\n",
    "1. We load source documents\n",
    "2. We split those source documents into smaller chunks (documents)\n",
    "3. We send each of those documents to the `text-embedding-3-small` OpenAI API endpoint\n",
    "4. We store each of the text representations with the vector representations as keys/values in a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-vWANZyR6yw"
   },
   "source": [
    "## Step 3: Finding Best Match with Semantic Similarity\n",
    "\n",
    "The next step is to be able to query our `VectorDatabase()` with a `str` and have it return to us vectors and text that is most relevant from our corpus.\n",
    "\n",
    "We're going to use the following process to achieve this in our toy example:\n",
    "\n",
    "1. We need to embed our query with the same `EmbeddingModel()` as we used to construct our `VectorDatabase()`\n",
    "2. We loop through every vector in our `VectorDatabase()` and use a distance measure to compare how related they are\n",
    "3. We return a list of the top `k` closest vectors, with their text representations\n",
    "\n",
    "There's some very heavy optimization that can be done at each of these steps - but let's just focus on the basic pattern in this notebook.\n",
    "\n",
    "> We are using [cosine similarity](https://www.engati.com/glossary/cosine-similarity) as a distance metric in this example - but there are many many distance metrics you could use - like [these](https://flavien-vidal.medium.com/similarity-distances-for-natural-language-processing-16f63cd5ba55)\n",
    "\n",
    "> We are using a rather inefficient way of calculating relative distance between the query vector and all other vectors - there are more advanced approaches that are much more efficient, like [ANN](https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76d96uavR6yw",
    "outputId": "bbfccc31-20a2-41c7-c14d-46554a43ed2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('f Capitan\\nTiago with a telegram, to open which he naturally requested the\\npermission of the others, who very naturally begged him to do so. The\\nworthy capitan at first knitted his eyebrows, then raised them;\\nhis face became pale, then lighted up as he hastily folded the paper\\nand arose.\\n\\n\"Gentlemen,\" he announced in confusion, \"his Excellency the\\nCaptain-General is coming this evening to honor my house.\" Thereupon he\\nset off at a run, hatless, taking with him the message and his napkin.\\n\\nHe was followed by exclamations and questions, for a cry of\\n\"Tulisanes!\" would not have produced greater effect. \"But,\\nlisten!\" \"When is he coming?\" \"Tell us about it!\" \"His Excellency!\" But\\nCapitan Tiago was already far away.\\n\\n\"His Excellency is coming and will stay at Capitan Tiago\\'s!\" exclaimed\\nsome without taking into consideration the fact that his daughter\\nand future son-in-law were present.\\n\\n\"The choice couldn\\'t be better,\" answered the latter.\\n\\nThe friars gazed at one another with looks that se',\n",
       "  np.float64(0.6044985971271886)),\n",
       " ('his cell, with his elbow upon the window\\nsill and his pale, worn cheek resting on the palm of his hand, he\\nwas gazing silently into the distance where a bright star glittered\\nin the dark sky. The star paled and disappeared, the dim light of the\\nwaning moon faded, but the friar did not move from his place--he was\\ngazing out over the field of Bagumbayan and the sleeping sea at the\\nfar horizon wrapped in the morning mist.\\n\\n\\n\\n\\nCHAPTER VI\\n\\nCapitan Tiago\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tThy will be done on earth.\\n\\n\\nWhile our characters are deep in slumber or busy with their breakfasts,\\nlet us turn our attention to Capitan Tiago. We have never had the\\nhonor of being his guest, so it is neither our right nor our duty to\\npass him by slightingly, even under the stress of important events.\\n\\nLow in stature, with a clear complexion, a corpulent figure and a\\nfull face, thanks to the liberal supply of fat which according to his\\nadmirers was the gift of Heaven and which his enemies averred was the\\nblood of the poor, Capi',\n",
       "  np.float64(0.4733489327607678)),\n",
       " ('pitan Tiago had great respect for this Chinese, who passed himself\\noff as a prophet and a physician. Examining the palm of the deceased\\nlady just before her daughter was born, he had prognosticated:\\n\"If it\\'s not a boy and doesn\\'t die, it\\'ll be a fine girl!\" [169] and\\nMaria Clara had come into the world to fulfill the infidel\\'s prophecy.\\n\\nCapitan Tiago, then, as a prudent and cautious man, could not decide\\nso easily as Trojan Paris--he could not so lightly give the preference\\nto one Virgin for fear of offending another, a situation that might be\\nfraught with grave consequences. \"Prudence!\" he said to himself. \"Let\\'s\\nnot go and spoil it all now.\"\\n\\nHe was still in the midst of these doubts when the governmental party\\narrived,--Doña Victorina, Don Tiburcio, and Linares. Doña Victorina did\\nthe talking for the three men as well as for herself. She mentioned\\nLinares\\' visits to the Captain-General and repeatedly insinuated\\nthe advantages of a relative of \"quality.\" \"Now,\" she concluded,\\n\"as we',\n",
       "  np.float64(0.47057724308214993)),\n",
       " ('sand pesos. Capitan Tiago hoped that the old woman\\nwould breathe her last almost any day, or that she would lose five or\\nsix of her lawsuits, so that he might be alone in serving God; but\\nunfortunately the best lawyers of the _Real Audiencia_ looked after\\nher interests, and as to her health, there was no part of her that\\ncould be attacked by sickness; she seemed to be a steel wire, no doubt\\nfor the edification of souls, and she hung on in this vale of tears\\nwith the tenacity of a boil on the skin. Her adherents were secure in\\nthe belief that she would be canonized at her death and that Capitan\\nTiago himself would have to worship her at the altars--all of which\\nhe agreed to and cheerfully promised, provided only that she die soon.\\n\\nSuch was Capitan Tiago in the days of which we write. As for the past,\\nhe was the only son of a sugar-planter of Malabon, wealthy enough,\\nbut so miserly that he would not spend a cent to educate his son,\\nfor which reason the little Santiago had been the serva',\n",
       "  np.float64(0.4509026521866564)),\n",
       " (\" of the rich guild of mestizos in spite of\\nthe protests of many of them, who did not regard him as one of\\nthemselves. In the two years that he held this office he wore out ten\\nfrock coats, an equal number of high hats, and half a dozen canes. The\\nfrock coat and the high hat were in evidence at the Ayuntamiento,\\nin the governor-general's palace, and at military headquarters; the\\nhigh hat and the frock coat might have been noticed in the cockpit,\\nin the market, in the processions, in the Chinese shops, and under the\\nhat and within the coat might have been seen the perspiring Capitan\\nTiago, waving his tasseled cane, directing, arranging, and throwing\\neverything into disorder with marvelous activity and a gravity even\\nmore marvelous.\\n\\nSo the authorities saw in him a safe man, gifted with the best of\\ndispositions, peaceful, tractable, and obsequious, who read no books\\nor newspapers from Spain, although he spoke Spanish well. Indeed,\\nthey rather looked upon him with the feeling with which a \",\n",
       "  np.float64(0.4392205490510855))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db.search_by_text(\"Who is Capitan Tiago?\", k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TehsfIiKR6yw"
   },
   "source": [
    "## Task 5: Generate the Response\n",
    "\n",
    "In the following section, we'll be looking at the role of prompts - and how they help us to guide our application in the right direction.\n",
    "\n",
    "In this notebook, we're going to rely on the idea of \"zero-shot in-context learning\".\n",
    "\n",
    "This is a lot of words to say: \"We will ask it to perform our desired task in the prompt, and provide no examples.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXpA0UveR6yw"
   },
   "source": [
    "### XYZRolePrompt\n",
    "\n",
    "Before we do that, let's stop and think a bit about how OpenAI's chat models work.\n",
    "\n",
    "We know they have roles - as is indicated in the following API [documentation](https://platform.openai.com/docs/api-reference/chat/create#chat/create-messages)\n",
    "\n",
    "There are three roles, and they function as follows (taken directly from [OpenAI](https://platform.openai.com/docs/guides/gpt/chat-completions-api)):\n",
    "\n",
    "- `{\"role\" : \"system\"}` : The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model’s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"\n",
    "- `{\"role\" : \"user\"}` : The user messages provide requests or comments for the assistant to respond to.\n",
    "- `{\"role\" : \"assistant\"}` : Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.\n",
    "\n",
    "The main idea is this:\n",
    "\n",
    "1. You start with a system message that outlines how the LLM should respond, what kind of behaviours you can expect from it, and more\n",
    "2. Then, you can provide a few examples in the form of \"assistant\"/\"user\" pairs\n",
    "3. Then, you prompt the model with the true \"user\" message.\n",
    "\n",
    "In this example, we'll be forgoing the 2nd step for simplicities sake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdZ2KWKSR6yw"
   },
   "source": [
    "#### Utility Functions\n",
    "\n",
    "You'll notice that we're using some utility functions from the `aimakerspace` module - let's take a peek at these and see what they're doing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFbeJDDsR6yw"
   },
   "source": [
    "##### XYZRolePrompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mojJSE3R6yw"
   },
   "source": [
    "Here we have our `system`, `user`, and `assistant` role prompts.\n",
    "\n",
    "Let's take a peek at what they look like:\n",
    "\n",
    "```python\n",
    "class BasePrompt:\n",
    "    def __init__(self, prompt):\n",
    "        \"\"\"\n",
    "        Initializes the BasePrompt object with a prompt template.\n",
    "\n",
    "        :param prompt: A string that can contain placeholders within curly braces\n",
    "        \"\"\"\n",
    "        self.prompt = prompt\n",
    "        self._pattern = re.compile(r\"\\{([^}]+)\\}\")\n",
    "\n",
    "    def format_prompt(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Formats the prompt string using the keyword arguments provided.\n",
    "\n",
    "        :param kwargs: The values to substitute into the prompt string\n",
    "        :return: The formatted prompt string\n",
    "        \"\"\"\n",
    "        matches = self._pattern.findall(self.prompt)\n",
    "        return self.prompt.format(**{match: kwargs.get(match, \"\") for match in matches})\n",
    "\n",
    "    def get_input_variables(self):\n",
    "        \"\"\"\n",
    "        Gets the list of input variable names from the prompt string.\n",
    "\n",
    "        :return: List of input variable names\n",
    "        \"\"\"\n",
    "        return self._pattern.findall(self.prompt)\n",
    "```\n",
    "\n",
    "Then we have our `RolePrompt` which laser focuses us on the role pattern found in most API endpoints for LLMs.\n",
    "\n",
    "```python\n",
    "class RolePrompt(BasePrompt):\n",
    "    def __init__(self, prompt, role: str):\n",
    "        \"\"\"\n",
    "        Initializes the RolePrompt object with a prompt template and a role.\n",
    "\n",
    "        :param prompt: A string that can contain placeholders within curly braces\n",
    "        :param role: The role for the message ('system', 'user', or 'assistant')\n",
    "        \"\"\"\n",
    "        super().__init__(prompt)\n",
    "        self.role = role\n",
    "\n",
    "    def create_message(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates a message dictionary with a role and a formatted message.\n",
    "\n",
    "        :param kwargs: The values to substitute into the prompt string\n",
    "        :return: Dictionary containing the role and the formatted message\n",
    "        \"\"\"\n",
    "        return {\"role\": self.role, \"content\": self.format_prompt(**kwargs)}\n",
    "```\n",
    "\n",
    "We'll look at how the `SystemRolePrompt` is constructed to get a better idea of how that extension works:\n",
    "\n",
    "```python\n",
    "class SystemRolePrompt(RolePrompt):\n",
    "    def __init__(self, prompt: str):\n",
    "        super().__init__(prompt, \"system\")\n",
    "```\n",
    "\n",
    "That pattern is repeated for our `UserRolePrompt` and our `AssistantRolePrompt` as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D361R6sMR6yw"
   },
   "source": [
    "##### ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJVQ2Pm8R6yw"
   },
   "source": [
    "Next we have our model, which is converted to a format analagous to libraries like LangChain and LlamaIndex.\n",
    "\n",
    "Let's take a peek at how that is constructed:\n",
    "\n",
    "```python\n",
    "class ChatOpenAI:\n",
    "    def __init__(self, model_name: str = \"gpt-4o-mini\"):\n",
    "        self.model_name = model_name\n",
    "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if self.openai_api_key is None:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "    def run(self, messages, text_only: bool = True):\n",
    "        if not isinstance(messages, list):\n",
    "            raise ValueError(\"messages must be a list\")\n",
    "\n",
    "        openai.api_key = self.openai_api_key\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model_name, messages=messages\n",
    "        )\n",
    "\n",
    "        if text_only:\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "        return response\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCU7FfhIR6yw"
   },
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### ❓ Question #3:\n",
    "\n",
    "When calling the OpenAI API - are there any ways we can achieve more reproducible outputs?\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### Answer:\n",
    "\n",
    "* To achieve more reproducible outputs across API calls, we can set the seed parameter to any integer of our choice and use the same value across requests we'd like to have deterministic outputs for. Additionally, ensure all other parameters, such as prompt or temperature, remain the same, and set top_p to a very low number. By following these steps, we can obtain reproducible results.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> NOTE: Check out [this section](https://platform.openai.com/docs/guides/text-generation/) of the OpenAI documentation for the answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5wcjMLCR6yw"
   },
   "source": [
    "### Creating and Prompting OpenAI's `gpt-4o-mini`!\n",
    "\n",
    "Let's tie all these together and use it to prompt `gpt-4o-mini`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WIfpIot7R6yw"
   },
   "outputs": [],
   "source": [
    "from psi.openai_utils.prompts import (\n",
    "    UserRolePrompt,\n",
    "    SystemRolePrompt,\n",
    "    AssistantRolePrompt,\n",
    ")\n",
    "\n",
    "from psi.openai_utils.chatmodel import ChatOpenAI\n",
    "\n",
    "chat_openai = ChatOpenAI()\n",
    "user_prompt_template = \"{content}\"\n",
    "user_role_prompt = UserRolePrompt(user_prompt_template)\n",
    "system_prompt_template = (\n",
    "    \"You are an expert in {expertise}, you always answer in a kind way.\"\n",
    ")\n",
    "system_role_prompt = SystemRolePrompt(system_prompt_template)\n",
    "\n",
    "messages = [\n",
    "    system_role_prompt.create_message(expertise=\"Python\"),\n",
    "    user_role_prompt.create_message(\n",
    "        content=\"What is the best way to write a loop?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = chat_openai.run(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dHo7lssNR6yw",
    "outputId": "1d3823fa-bb6b-45f6-ddba-b41686388324"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing a loop in Python can vary based on the context and the specific task you want to achieve. Here are a few common ways to write loops, along with some tips to make them effective and readable.\n",
      "\n",
      "### 1. `for` Loop\n",
      "A `for` loop is commonly used for iterating over a sequence (like a list, tuple, or string).\n",
      "\n",
      "```python\n",
      "# Example: Looping through a list\n",
      "fruits = ['apple', 'banana', 'cherry']\n",
      "for fruit in fruits:\n",
      "    print(fruit)\n",
      "```\n",
      "\n",
      "### 2. `while` Loop\n",
      "A `while` loop continues until a certain condition is false.\n",
      "\n",
      "```python\n",
      "# Example: Using a while loop\n",
      "count = 0\n",
      "while count < 3:\n",
      "    print(\"Count is:\", count)\n",
      "    count += 1\n",
      "```\n",
      "\n",
      "### 3. Using `enumerate()`\n",
      "When you need both the index and the value from a list, `enumerate()` can be very handy.\n",
      "\n",
      "```python\n",
      "# Example: Looping with index\n",
      "fruits = ['apple', 'banana', 'cherry']\n",
      "for index, fruit in enumerate(fruits):\n",
      "    print(index, fruit)\n",
      "```\n",
      "\n",
      "### 4. List Comprehensions\n",
      "If you're looking to create a new list based on an existing one, consider using list comprehensions instead of loops.\n",
      "\n",
      "```python\n",
      "# Example: List comprehension\n",
      "squared_numbers = [x**2 for x in range(10)]\n",
      "print(squared_numbers)\n",
      "```\n",
      "\n",
      "### Tips for Writing Effective Loops\n",
      "- **Clarity over Complexity**: Choose clarity. Readable code is more important than conciseness.\n",
      "- **Avoid Infinite Loops**: Ensure the condition in a while loop will eventually become false.\n",
      "- **Use `break` and `continue` Wisely**: These can control loop flow but can make code harder to read if overused.\n",
      "- **Use Built-in Functions**: Functions like `map()`, `filter()`, or `reduce()` can sometimes replace loops altogether and may be more efficient.\n",
      "\n",
      "### Example Loop with Conditions\n",
      "Here’s a slightly more complex example that demonstrates a loop with conditions:\n",
      "\n",
      "```python\n",
      "# Example: Filtering and processing in a loop\n",
      "numbers = [1, 2, 3, 4, 5, 6]\n",
      "for num in numbers:\n",
      "    if num % 2 == 0:\n",
      "        print(num, \"is even\")\n",
      "    else:\n",
      "        print(num, \"is odd\")\n",
      "```\n",
      "\n",
      "Choosing the right type of loop and writing it clearly will depend on your specific situation. Feel free to ask if you need further clarification or examples! Happy coding!\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2nxxhB2R6yy"
   },
   "source": [
    "## Task 5: Retrieval Augmented Generation\n",
    "\n",
    "Now we can create a RAG prompt - which will help our system behave in a way that makes sense!\n",
    "\n",
    "There is much you could do here, many tweaks and improvements to be made!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "D1hamzGaR6yy"
   },
   "outputs": [],
   "source": [
    "RAG_SYSTEM_TEMPLATE = \"\"\"You are a knowledgeable assistant that answers questions based strictly on provided context.\n",
    "\n",
    "Instructions:\n",
    "- Only answer questions using information from the provided context\n",
    "- If the context doesn't contain relevant information, respond with \"I don't know\"\n",
    "- Be accurate and cite specific parts of the context when possible\n",
    "- Keep responses {response_style} and {response_length}\n",
    "- Only use the provided context. Do not use external knowledge.\n",
    "- Only provide answers when you are confident the context supports your response.\"\"\"\n",
    "\n",
    "RAG_USER_TEMPLATE = \"\"\"Context Information:\n",
    "{context}\n",
    "\n",
    "Number of relevant sources found: {context_count}\n",
    "{similarity_scores}\n",
    "\n",
    "Question: {user_query}\n",
    "\n",
    "Please provide your answer based solely on the context above.\"\"\"\n",
    "\n",
    "rag_system_prompt = SystemRolePrompt(\n",
    "    RAG_SYSTEM_TEMPLATE,\n",
    "    strict=True,\n",
    "    defaults={\n",
    "        \"response_style\": \"concise\",\n",
    "        \"response_length\": \"brief\"\n",
    "    }\n",
    ")\n",
    "\n",
    "rag_user_prompt = UserRolePrompt(\n",
    "    RAG_USER_TEMPLATE,\n",
    "    strict=True,\n",
    "    defaults={\n",
    "        \"context_count\": \"\",\n",
    "        \"similarity_scores\": \"\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalAugmentedQAPipeline:\n",
    "    def __init__(self, llm: ChatOpenAI(), vector_db_retriever: VectorDatabase, \n",
    "                 response_style: str = \"detailed\", include_scores: bool = False) -> None:\n",
    "        self.llm = llm\n",
    "        self.vector_db_retriever = vector_db_retriever\n",
    "        self.response_style = response_style\n",
    "        self.include_scores = include_scores\n",
    "\n",
    "    def run_pipeline(self, user_query: str, k: int = 4, **system_kwargs) -> dict:\n",
    "        # Retrieve relevant contexts\n",
    "        context_list = self.vector_db_retriever.search_by_text(user_query, k=k)\n",
    "        \n",
    "        context_prompt = \"\"\n",
    "        similarity_scores = []\n",
    "        \n",
    "        for i, (context, score) in enumerate(context_list, 1):\n",
    "            context_prompt += f\"[Source {i}]: {context}\\n\\n\"\n",
    "            similarity_scores.append(f\"Source {i}: {score:.3f}\")\n",
    "        \n",
    "        # Create system message with parameters\n",
    "        system_params = {\n",
    "            \"response_style\": self.response_style,\n",
    "            \"response_length\": system_kwargs.get(\"response_length\", \"detailed\")\n",
    "        }\n",
    "        \n",
    "        formatted_system_prompt = rag_system_prompt.create_message(**system_params)\n",
    "        \n",
    "        user_params = {\n",
    "            \"user_query\": user_query,\n",
    "            \"context\": context_prompt.strip(),\n",
    "            \"context_count\": len(context_list),\n",
    "            \"similarity_scores\": f\"Relevance scores: {', '.join(similarity_scores)}\" if self.include_scores else \"\"\n",
    "        }\n",
    "        \n",
    "        formatted_user_prompt = rag_user_prompt.create_message(**user_params)\n",
    "\n",
    "        return {\n",
    "            \"response\": self.llm.run([formatted_system_prompt, formatted_user_prompt]), \n",
    "            \"context\": context_list,\n",
    "            \"context_count\": len(context_list),\n",
    "            \"similarity_scores\": similarity_scores if self.include_scores else None,\n",
    "            \"prompts_used\": {\n",
    "                \"system\": formatted_system_prompt,\n",
    "                \"user\": formatted_user_prompt\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Capitan Tiago is a character described as low in stature, with a clear complexion, a corpulent figure, and a full face, which some attribute to a \"liberal supply of fat,\" seen by his admirers as a gift from Heaven and by his enemies as the result of exploiting the poor (Source 2). He is depicted as a somewhat cautious and prudent man, particularly in the context of social interactions and decisions he must make concerning his daughter, Maria Clara. Capitan Tiago has respect for a Chinese man who claims to be a prophet and a physician, having predicted the birth of Maria Clara (Source 3).\n",
      "\n",
      "He is also noted for hosting an important visitor, the Captain-General, who is expected to arrive at his house, an announcement that causes excitement among others present (Source 1). Overall, he is a significant figure in his community, involved in social and governmental affairs highlighted by the presence of guests from the government (Source 3).\n",
      "\n",
      "Context Count: 3\n",
      "Similarity Scores: ['Source 1: 0.604', 'Source 2: 0.473', 'Source 3: 0.471']\n"
     ]
    }
   ],
   "source": [
    "rag_pipeline = RetrievalAugmentedQAPipeline(\n",
    "    vector_db_retriever=vector_db,\n",
    "    llm=chat_openai,\n",
    "    response_style=\"detailed\",\n",
    "    include_scores=True\n",
    ")\n",
    "\n",
    "result = rag_pipeline.run_pipeline(\n",
    "    \"Who is Capitan Tiago?\",\n",
    "    k=3,\n",
    "    response_length=\"comprehensive\", \n",
    "    include_warnings=True,\n",
    "    confidence_required=True\n",
    ")\n",
    "\n",
    "print(f\"Response: {result['response']}\")\n",
    "print(f\"\\nContext Count: {result['context_count']}\")\n",
    "print(f\"Similarity Scores: {result['similarity_scores']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### 🎯 Breakout Room - Group Discussion: \n",
    "\n",
    "Explore different prompts and check if RAG is able to correctly answer the question. Write down final answer below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### Observations: \n",
    "\n",
    "* The RAG system follows the constraints, citing its context and responding with \"I don't know\" when the answer is not found in the retrieved chunks. Furthermore, the RAG system adheres to its established RAG system and user prompt template, including its behavior and response structure. The RAG system successfully answered the specified question.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "02-embeddings-and-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1ce393d9afcf427d9d352259c5d32678": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e6efd99f7d346e485b002fb0fa85cc7",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3dfb67c39958461da6071e4c19c3fa41",
      "value": 1
     }
    },
    "3a4ba348cb004f8ab7b2b1395539c81b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d2ea5009dd16442cb5d8a0ac468e50a8",
      "placeholder": "​",
      "style": "IPY_MODEL_5f00135fe1044051a50ee5e841cbb8e3",
      "value": "0.018 MB of 0.018 MB uploaded\r"
     }
    },
    "3dfb67c39958461da6071e4c19c3fa41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4e6efd99f7d346e485b002fb0fa85cc7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56a8e24025594e5e9ff3b8581c344691": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f00135fe1044051a50ee5e841cbb8e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bb904e05ece143c79ecc4f20de482f45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3a4ba348cb004f8ab7b2b1395539c81b",
       "IPY_MODEL_1ce393d9afcf427d9d352259c5d32678"
      ],
      "layout": "IPY_MODEL_56a8e24025594e5e9ff3b8581c344691"
     }
    },
    "d2ea5009dd16442cb5d8a0ac468e50a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
